{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  8232320\n",
      "Features shape: torch.Size([50, 256, 10, 10])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'densetFPN_121' object has no attribute 'conv_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Print output shapes to verify\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_info\u001b[49m())\n",
      "File \u001b[1;32md:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'densetFPN_121' object has no attribute 'conv_info'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class densetFPN_121(nn.Module):\n",
    "    \"\"\" DenseNet121-based Feature Pyramid Network (FPN) for feature extraction. \n",
    "        Total number of parameters:  8232320 (8.23 million) \"\"\" \n",
    "    def __init__(self, weights='DEFAULT', common_channel_size=256, output_channel_size=256):\n",
    "        super(densetFPN_121, self).__init__()\n",
    "        original_densenet = models.densenet121(weights=weights)\n",
    "        \n",
    "        # Initial layers: extract features without modification\n",
    "        self.encoder = nn.ModuleList([\n",
    "            nn.Sequential(*list(original_densenet.features.children())[:6], nn.Dropout(0.4)),   # 128x12x12\n",
    "            nn.Sequential(*list(original_densenet.features.children())[6:8], nn.Dropout(0.4)),  # 256x6x6\n",
    "            nn.Sequential(*list(original_densenet.features.children())[8:10], nn.Dropout(0.4)), # 896x3x3\n",
    "            nn.Sequential(*list(original_densenet.features.children())[10:-1], nn.Dropout(0.4)) # 1920x3x3\n",
    "        ])\n",
    "        \n",
    "        # Define convolutional layers for adapting channel sizes\n",
    "        fpn_channels = [128, 256, 512, 1024]\n",
    "        self.adaptation_layers = nn.ModuleDict({\n",
    "            f'adapt{i+1}': nn.Conv2d(fpn_channels[i], common_channel_size, kernel_size=1)\n",
    "            for i in range(4)\n",
    "        })\n",
    "\n",
    "        # Define FPN layers\n",
    "        self.fpn = nn.ModuleDict({\n",
    "            f'fpn{i+1}': nn.Conv2d(common_channel_size, common_channel_size, kernel_size=1)\n",
    "            for i in range(3)\n",
    "        })\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Conv2d(common_channel_size, output_channel_size, kernel_size=3), # kernel size 1 or 3\n",
    "            nn.BatchNorm2d(output_channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4) # 0.2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for encoder in self.encoder:\n",
    "            x = encoder(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Merge channels using 1x1 convolutions\n",
    "        adapted_features = [self.adaptation_layers[f'adapt{i+1}'](features[i]) for i in range(4)]\n",
    "        \n",
    "        # FPN integration using top-down pathway\n",
    "        fpn_output = adapted_features.pop()  # Start with the deepest features\n",
    "        for i in reversed(range(3)):\n",
    "            upsampled = F.interpolate(fpn_output, size=adapted_features[i].shape[-2:], mode='nearest')\n",
    "            fpn_output = self.fpn[f'fpn{i+1}'](upsampled + adapted_features[i])\n",
    "        \n",
    "        # Merge features\n",
    "        merged_features = self.merge_layers(fpn_output)\n",
    "        \n",
    "        return merged_features\n",
    "\n",
    "\n",
    "model = densetFPN_121(weights=None)\n",
    "\n",
    "# Print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "# Create a dummy input tensor of size [50, 3, 100, 100]\n",
    "dummy_input = torch.randn(50, 3, 100, 100)\n",
    "\n",
    "# Forward pass through the model with dummy input\n",
    "features = model(dummy_input)\n",
    "\n",
    "# Print output shapes to verify\n",
    "print(\"Features shape:\", features.shape)\n",
    "\n",
    "# print(features.conv_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  20971120\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class efficientDecoder_v2_s(nn.Module):\n",
    "    def __init__(self, num_channels=[24, 48, 64, 128, 160, 256], output_channel_size=256, output_feature_size=25):\n",
    "        super(efficientDecoder_v2_s, self).__init__()\n",
    "        # Load EfficientNet V2 Small features\n",
    "        efficientnet_v2_s = models.efficientnet_v2_s(weights='DEFAULT').features[:-1]\n",
    "\n",
    "        # Modularize encoders\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[:2], nn.Dropout(0.1)),    # 24x50x50\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[2:3], nn.Dropout(0.1)),   # 48x25x25\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[3:4], nn.Dropout(0.2)),   # 64x13x13\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[4:5], nn.Dropout(0.2)),   # 128x7x7\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[5:6], nn.Dropout(0.3)),   # 160x7x7 # TODO: Check whether to skip 128x7x7\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[6:7], nn.Dropout(0.3))    # 256x4x4\n",
    "        ])\n",
    "        \n",
    "        # Modularize upconvolutions\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=160, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.Conv2d(in_channels=160, out_channels=128, kernel_size=1, stride=1),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=48, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(in_channels=48, out_channels=24, kernel_size=2, stride=2)\n",
    "        ])\n",
    "        \n",
    "        # Modularize decoders\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(num_channels[i] * 2, num_channels[i], kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(num_channels[i]),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(0.1 + i * 0.05)\n",
    "            ) for i in range(len(num_channels)-1)\n",
    "        ])\n",
    "\n",
    "        # Optional, merge layers to increase the number of channels\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Conv2d(24, output_channel_size, kernel_size=1),\n",
    "            nn.BatchNorm2d(output_channel_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(output_feature_size) # to reduce noise and overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = features.pop()\n",
    "        for upconv, decoder, feature in zip(self.upconvs, reversed(self.decoders), reversed(features)):\n",
    "            x = upconv(x)\n",
    "            x = torch.cat((x, feature), dim=1)\n",
    "            x = decoder(x)\n",
    "        \n",
    "        x = self.merge_layers(x) # Introduced to increase the number of channels\n",
    "        pooled_features = self.global_avg_pool(x) # Introduced to reduce noise and overfitting\n",
    "        \n",
    "        return pooled_features\n",
    "\n",
    "model = efficientDecoder_v2_s()\n",
    "\n",
    "# Print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "# Create a dummy input tensor of size [50, 3, 100, 100]\n",
    "dummy_input = torch.randn(50, 3, 100, 100)\n",
    "\n",
    "# Forward pass through the model with dummy input\n",
    "features = model(dummy_input)\n",
    "\n",
    "# Print output shapes to verify\n",
    "print(\"Features shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed(range(1,6)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  20971120\n",
      "Output shape: torch.Size([10, 256, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class efficientDecoder_v2_s(nn.Module):\n",
    "    def __init__(self, output_channel_size=256, output_feature_size=25):\n",
    "        super(efficientDecoder_v2_s, self).__init__()\n",
    "        # Load EfficientNet V2 Small features\n",
    "        efficientnet_v2_s = models.efficientnet_v2_s(weights='DEFAULT').features[:-1]\n",
    "\n",
    "        # Modularize encoders\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[:2], nn.Dropout(0.1)),    # 24x50x50\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[2:3], nn.Dropout(0.1)),   # 48x25x25\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[3:4], nn.Dropout(0.2)),   # 64x13x13\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[4:5], nn.Dropout(0.2)),   # 128x7x7\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[5:6], nn.Dropout(0.3)),   # 160x7x7\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[6:7], nn.Dropout(0.3))    # 256x4x4\n",
    "        ])\n",
    "        \n",
    "        # Modularize upconvolutions\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=160, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.Conv2d(in_channels=160, out_channels=128, kernel_size=1, stride=1),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=48, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(in_channels=48, out_channels=24, kernel_size=2, stride=2)\n",
    "        ])\n",
    "        \n",
    "        # Modularize decoders\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(160*2, 160, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.3, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128*2, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.3, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64*2, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.2, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(48*2, 48, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.2, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(24*2, 24, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.1, inplace=True)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # Optional, merge layers to increase the number of channels\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Conv2d(24, output_channel_size, kernel_size=1),\n",
    "            nn.BatchNorm2d(output_channel_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(output_feature_size) # to reduce noise and overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = features.pop()\n",
    "        for upconv, decoder, feature in zip(self.upconvs, self.decoders, reversed(features)):\n",
    "            x = upconv(x)\n",
    "            x = torch.cat((x, feature), dim=1)\n",
    "            x = decoder(x)\n",
    "        \n",
    "        x = self.merge_layers(x) # Introduced to increase the number of channels\n",
    "        pooled_features = self.global_avg_pool(x) # Introduced to reduce noise and overfitting\n",
    "        \n",
    "        return pooled_features\n",
    "\n",
    "model = efficientDecoder_v2_s()\n",
    "\n",
    "# Print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "# Example initialization and forward pass\n",
    "# model = EfficientDecoder_v2_s()\n",
    "dummy_input = torch.randn(10, 3, 100, 100)  # Adjust size according to your actual input\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jerem\\Desktop\\ICL-MSc\\Final_Year_Project\\FYP-interpretable-deep-learning\\2D-model\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 13 but got size 14 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m model \u001b[38;5;241m=\u001b[39m construct_baselineModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefficientDecoder_v2_s\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m, in \u001b[0;36mBaselineModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madaptive_pool(x)\n\u001b[0;32m     50\u001b[0m     intermediate_outputs \u001b[38;5;241m=\u001b[39m [layer(x) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_specific_layers]\n",
      "File \u001b[1;32md:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jerem\\Desktop\\ICL-MSc\\Final_Year_Project\\FYP-interpretable-deep-learning\\2D-model\\src\\models\\feature_extractors.py:264\u001b[0m, in \u001b[0;36mefficientDecoder_v2_s.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m upconv, decoder, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders, \u001b[38;5;28mreversed\u001b[39m(features)):\n\u001b[0;32m    263\u001b[0m     x \u001b[38;5;241m=\u001b[39m upconv(x)\n\u001b[1;32m--> 264\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     x \u001b[38;5;241m=\u001b[39m decoder(x)\n\u001b[0;32m    267\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_layers(x) \u001b[38;5;66;03m# Introduced to increase the number of channels\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 13 but got size 14 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys, os\n",
    "#print(os.getcwd())\n",
    "from feature_extractors import densetFPN_121, densetFPN_201, efficientFPN_v2_s, efficientDecoder_v2_s\n",
    "\n",
    "# Define the dictionary outside the class\n",
    "MODEL_DICT = {\n",
    "    'densetFPN_121': densetFPN_121,\n",
    "    'densetFPN_201': densetFPN_201,\n",
    "    'efficientFPN_v2_s': efficientFPN_v2_s,\n",
    "    'efficientDecoder_v2_s': efficientDecoder_v2_s\n",
    "}\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, backbone, num_tasks=5, feature_dim=(256, 25, 25)):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.backbone = backbone()  # Instantiate the backbone passed as a class\n",
    "        \n",
    "        feature_channels, feature_width, feature_height = feature_dim\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((feature_width, feature_height))\n",
    "        \n",
    "        self.task_specific_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(feature_channels * feature_width * feature_height, 1024),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        self.task_specific_classifier = nn.ModuleList([\n",
    "            nn.Linear(1024, 1) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(1024 * num_tasks, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        intermediate_outputs = [layer(x) for layer in self.task_specific_layers]\n",
    "        concatenated_outputs = torch.cat(intermediate_outputs, dim=1)\n",
    "        \n",
    "        task_outputs = [torch.sigmoid(classifier(io)) for io, classifier in zip(intermediate_outputs, self.task_specific_classifier)]\n",
    "        \n",
    "        final_output = torch.sigmoid(self.final_classifier(concatenated_outputs))\n",
    "        \n",
    "        return final_output, task_outputs\n",
    "\n",
    "def construct_baselineModel(model_name, num_tasks=5, feature_dim=(256, 25, 25)):\n",
    "    if model_name not in MODEL_DICT:\n",
    "        raise ValueError(f\"Unsupported model name {model_name}\")\n",
    "    backbone = MODEL_DICT[model_name]\n",
    "    return BaselineModel(backbone, num_tasks, feature_dim)\n",
    "\n",
    "# Example usage\n",
    "model = construct_baselineModel('efficientDecoder_v2_s')\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "num_channels=[24, 48, 64, 128, 160, 256]\n",
    "for i in reversed(range(3)):\n",
    "    print(i)\n",
    "    \n",
    "for i in range(3):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
