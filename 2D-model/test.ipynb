{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class densetFPN_121(nn.Module):\n",
    "    \"\"\" DenseNet121-based Feature Pyramid Network (FPN) for feature extraction. \n",
    "        Total number of parameters:  8232320 (8.23 million) \"\"\" \n",
    "    def __init__(self, weights='DEFAULT', common_channel_size=256, output_channel_size=256):\n",
    "        super(densetFPN_121, self).__init__()\n",
    "        original_densenet = models.densenet121(weights=weights)\n",
    "        \n",
    "        # Initial layers: extract features without modification\n",
    "        self.encoder = nn.ModuleList([\n",
    "            nn.Sequential(*list(original_densenet.features.children())[:6], nn.Dropout(0.4)),   # 128x12x12\n",
    "            nn.Sequential(*list(original_densenet.features.children())[6:8], nn.Dropout(0.4)),  # 256x6x6\n",
    "            nn.Sequential(*list(original_densenet.features.children())[8:10], nn.Dropout(0.4)), # 896x3x3\n",
    "            nn.Sequential(*list(original_densenet.features.children())[10:-1], nn.Dropout(0.4)) # 1920x3x3\n",
    "        ])\n",
    "        \n",
    "        # Define convolutional layers for adapting channel sizes\n",
    "        fpn_channels = [128, 256, 512, 1024]\n",
    "        self.adaptation_layers = nn.ModuleDict({\n",
    "            f'adapt{i+1}': nn.Conv2d(fpn_channels[i], common_channel_size, kernel_size=1)\n",
    "            for i in range(4)\n",
    "        })\n",
    "\n",
    "        # Define FPN layers\n",
    "        self.fpn = nn.ModuleDict({\n",
    "            f'fpn{i+1}': nn.Conv2d(common_channel_size, common_channel_size, kernel_size=1)\n",
    "            for i in range(3)\n",
    "        })\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Conv2d(common_channel_size, output_channel_size, kernel_size=3), # kernel size 1 or 3\n",
    "            nn.BatchNorm2d(output_channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4) # 0.2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for encoder in self.encoder:\n",
    "            x = encoder(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Merge channels using 1x1 convolutions\n",
    "        adapted_features = [self.adaptation_layers[f'adapt{i+1}'](features[i]) for i in range(4)]\n",
    "        \n",
    "        # FPN integration using top-down pathway\n",
    "        fpn_output = adapted_features.pop()  # Start with the deepest features\n",
    "        for i in reversed(range(3)):\n",
    "            upsampled = F.interpolate(fpn_output, size=adapted_features[i].shape[-2:], mode='nearest')\n",
    "            fpn_output = self.fpn[f'fpn{i+1}'](upsampled + adapted_features[i])\n",
    "        \n",
    "        # Merge features\n",
    "        merged_features = self.merge_layers(fpn_output)\n",
    "        \n",
    "        return merged_features\n",
    "\n",
    "\n",
    "model = densetFPN_121(weights=None)\n",
    "\n",
    "# Print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "# Create a dummy input tensor of size [50, 3, 100, 100]\n",
    "dummy_input = torch.randn(50, 3, 100, 100)\n",
    "\n",
    "# Forward pass through the model with dummy input\n",
    "features = model(dummy_input)\n",
    "\n",
    "# Print output shapes to verify\n",
    "print(\"Features shape:\", features.shape)\n",
    "\n",
    "# print(features.conv_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class efficientDecoder_v2_s(nn.Module):\n",
    "    def __init__(self, num_channels=[24, 48, 64, 128, 160, 256], output_channel_size=256, output_feature_size=25):\n",
    "        super(efficientDecoder_v2_s, self).__init__()\n",
    "        # Load EfficientNet V2 Small features\n",
    "        efficientnet_v2_s = models.efficientnet_v2_s(weights='DEFAULT').features[:-1]\n",
    "\n",
    "        # Modularize encoders\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[:2], nn.Dropout(0.1)),    # 24x50x50\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[2:3], nn.Dropout(0.1)),   # 48x25x25\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[3:4], nn.Dropout(0.2)),   # 64x13x13\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[4:5], nn.Dropout(0.2)),   # 128x7x7\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[5:6], nn.Dropout(0.3)),   # 160x7x7 # TODO: Check whether to skip 128x7x7\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[6:7], nn.Dropout(0.3))    # 256x4x4\n",
    "        ])\n",
    "        \n",
    "        # Modularize upconvolutions\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=160, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.Conv2d(in_channels=160, out_channels=128, kernel_size=1, stride=1),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=48, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(in_channels=48, out_channels=24, kernel_size=2, stride=2)\n",
    "        ])\n",
    "        \n",
    "        # Modularize decoders\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(num_channels[i] * 2, num_channels[i], kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(num_channels[i]),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(0.1 + i * 0.05)\n",
    "            ) for i in range(len(num_channels)-1)\n",
    "        ])\n",
    "\n",
    "        # Optional, merge layers to increase the number of channels\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Conv2d(24, output_channel_size, kernel_size=1),\n",
    "            nn.BatchNorm2d(output_channel_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(output_feature_size) # to reduce noise and overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = features.pop()\n",
    "        for upconv, decoder, feature in zip(self.upconvs, reversed(self.decoders), reversed(features)):\n",
    "            x = upconv(x)\n",
    "            x = torch.cat((x, feature), dim=1)\n",
    "            x = decoder(x)\n",
    "        \n",
    "        x = self.merge_layers(x) # Introduced to increase the number of channels\n",
    "        pooled_features = self.global_avg_pool(x) # Introduced to reduce noise and overfitting\n",
    "        \n",
    "        return pooled_features\n",
    "\n",
    "model = efficientDecoder_v2_s()\n",
    "\n",
    "# Print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "# Create a dummy input tensor of size [50, 3, 100, 100]\n",
    "dummy_input = torch.randn(50, 3, 100, 100)\n",
    "\n",
    "# Forward pass through the model with dummy input\n",
    "features = model(dummy_input)\n",
    "\n",
    "# Print output shapes to verify\n",
    "print(\"Features shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed(range(1,6)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class efficientDecoder_v2_s(nn.Module):\n",
    "    def __init__(self, output_channel_size=256, output_feature_size=25):\n",
    "        super(efficientDecoder_v2_s, self).__init__()\n",
    "        # Load EfficientNet V2 Small features\n",
    "        efficientnet_v2_s = models.efficientnet_v2_s(weights='DEFAULT').features[:-1]\n",
    "\n",
    "        # Modularize encoders\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[:2], nn.Dropout(0.1)),    # 24x50x50\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[2:3], nn.Dropout(0.1)),   # 48x25x25\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[3:4], nn.Dropout(0.2)),   # 64x13x13\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[4:5], nn.Dropout(0.2)),   # 128x7x7\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[5:6], nn.Dropout(0.3)),   # 160x7x7\n",
    "            nn.Sequential(*list(efficientnet_v2_s.children())[6:7], nn.Dropout(0.3))    # 256x4x4\n",
    "        ])\n",
    "        \n",
    "        # Modularize upconvolutions\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=160, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.Conv2d(in_channels=160, out_channels=128, kernel_size=1, stride=1),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=48, kernel_size=2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(in_channels=48, out_channels=24, kernel_size=2, stride=2)\n",
    "        ])\n",
    "        \n",
    "        # Modularize decoders\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(160*2, 160, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.3, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128*2, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.3, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64*2, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.2, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(48*2, 48, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.2, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(24*2, 24, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.1, inplace=True)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # Optional, merge layers to increase the number of channels\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Conv2d(24, output_channel_size, kernel_size=1),\n",
    "            nn.BatchNorm2d(output_channel_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(output_feature_size) # to reduce noise and overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = features.pop()\n",
    "        for upconv, decoder, feature in zip(self.upconvs, self.decoders, reversed(features)):\n",
    "            x = upconv(x)\n",
    "            x = torch.cat((x, feature), dim=1)\n",
    "            x = decoder(x)\n",
    "        \n",
    "        x = self.merge_layers(x) # Introduced to increase the number of channels\n",
    "        pooled_features = self.global_avg_pool(x) # Introduced to reduce noise and overfitting\n",
    "        \n",
    "        return pooled_features\n",
    "\n",
    "model = efficientDecoder_v2_s()\n",
    "\n",
    "# Print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "# Example initialization and forward pass\n",
    "# model = EfficientDecoder_v2_s()\n",
    "dummy_input = torch.randn(10, 3, 100, 100)  # Adjust size according to your actual input\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 12 12\n"
     ]
    }
   ],
   "source": [
    "input_dim=(128,12,12)\n",
    "\n",
    "input_channels, input_height, input_width = input_dim\n",
    "\n",
    "print(input_channels, input_height, input_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  45460865\n",
      "torch.Size([50, 3, 100, 100])\n",
      "torch.Size([50, 256, 12, 12])\n",
      "Features shape: torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.models.baseline_models import construct_baselineModel, construct_baseModel, BaseModel\n",
    "from src.models.backbone_models import densetFPN_121, densetFPN_201\n",
    "\n",
    "# model = densetFPN_121()\n",
    "# Create the model instance\n",
    "# model = BaseModel(backbone=densetFPN_121, weights='DEFAULT', input_dim=(256,12,12))\n",
    "model = construct_baseModel(backbone_name='densetFPN_121', weights='DEFAULT', input_dim=(256,12,12))\n",
    "\n",
    "# Print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "# Create a dummy input tensor of size [50, 3, 100, 100]\n",
    "dummy_input = torch.randn(50, 3, 100, 100)\n",
    "\n",
    "# Forward pass through the model with dummy input\n",
    "features = model(dummy_input)\n",
    "\n",
    "# Print output shapes to verify\n",
    "print(\"Features shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5106],\n",
       "         [0.5035],\n",
       "         [0.5488],\n",
       "         [0.7421],\n",
       "         [0.5720],\n",
       "         [0.3836],\n",
       "         [0.5911],\n",
       "         [0.4598],\n",
       "         [0.5526],\n",
       "         [0.5835],\n",
       "         [0.4567],\n",
       "         [0.5489],\n",
       "         [0.5642],\n",
       "         [0.4666],\n",
       "         [0.4395],\n",
       "         [0.6266],\n",
       "         [0.4210],\n",
       "         [0.5086],\n",
       "         [0.5422],\n",
       "         [0.5515],\n",
       "         [0.5906],\n",
       "         [0.6073],\n",
       "         [0.4439],\n",
       "         [0.5127],\n",
       "         [0.3540],\n",
       "         [0.6443],\n",
       "         [0.3925],\n",
       "         [0.5958],\n",
       "         [0.4544],\n",
       "         [0.4678],\n",
       "         [0.4648],\n",
       "         [0.5237],\n",
       "         [0.6828],\n",
       "         [0.5871],\n",
       "         [0.5303],\n",
       "         [0.5280],\n",
       "         [0.5333],\n",
       "         [0.6036],\n",
       "         [0.6479],\n",
       "         [0.5992],\n",
       "         [0.5446],\n",
       "         [0.5203],\n",
       "         [0.4284],\n",
       "         [0.5404],\n",
       "         [0.3937],\n",
       "         [0.5419],\n",
       "         [0.5040],\n",
       "         [0.5163],\n",
       "         [0.6542],\n",
       "         [0.6807]], grad_fn=<SigmoidBackward0>),\n",
       " [tensor([[0.3055],\n",
       "          [0.5454],\n",
       "          [0.4466],\n",
       "          [0.6103],\n",
       "          [0.6001],\n",
       "          [0.3678],\n",
       "          [0.5986],\n",
       "          [0.6536],\n",
       "          [0.4881],\n",
       "          [0.5223],\n",
       "          [0.4796],\n",
       "          [0.5688],\n",
       "          [0.4172],\n",
       "          [0.5538],\n",
       "          [0.5178],\n",
       "          [0.4031],\n",
       "          [0.3373],\n",
       "          [0.4337],\n",
       "          [0.4974],\n",
       "          [0.5876],\n",
       "          [0.4776],\n",
       "          [0.4990],\n",
       "          [0.4690],\n",
       "          [0.3868],\n",
       "          [0.4190],\n",
       "          [0.4670],\n",
       "          [0.4491],\n",
       "          [0.4863],\n",
       "          [0.6486],\n",
       "          [0.5797],\n",
       "          [0.5871],\n",
       "          [0.4863],\n",
       "          [0.6591],\n",
       "          [0.6616],\n",
       "          [0.5327],\n",
       "          [0.5821],\n",
       "          [0.4675],\n",
       "          [0.4971],\n",
       "          [0.5587],\n",
       "          [0.5801],\n",
       "          [0.4372],\n",
       "          [0.4937],\n",
       "          [0.5283],\n",
       "          [0.6593],\n",
       "          [0.5816],\n",
       "          [0.4639],\n",
       "          [0.4616],\n",
       "          [0.4363],\n",
       "          [0.5333],\n",
       "          [0.4207]], grad_fn=<SigmoidBackward0>),\n",
       "  tensor([[0.5994],\n",
       "          [0.4221],\n",
       "          [0.5482],\n",
       "          [0.5027],\n",
       "          [0.6056],\n",
       "          [0.4320],\n",
       "          [0.6220],\n",
       "          [0.3703],\n",
       "          [0.5256],\n",
       "          [0.4440],\n",
       "          [0.4216],\n",
       "          [0.6132],\n",
       "          [0.3906],\n",
       "          [0.5134],\n",
       "          [0.4589],\n",
       "          [0.5386],\n",
       "          [0.6318],\n",
       "          [0.5214],\n",
       "          [0.4835],\n",
       "          [0.5464],\n",
       "          [0.5554],\n",
       "          [0.6127],\n",
       "          [0.6334],\n",
       "          [0.4757],\n",
       "          [0.4426],\n",
       "          [0.4318],\n",
       "          [0.4770],\n",
       "          [0.6538],\n",
       "          [0.5836],\n",
       "          [0.3359],\n",
       "          [0.4776],\n",
       "          [0.5106],\n",
       "          [0.6561],\n",
       "          [0.3689],\n",
       "          [0.6169],\n",
       "          [0.5105],\n",
       "          [0.3370],\n",
       "          [0.6534],\n",
       "          [0.5109],\n",
       "          [0.3918],\n",
       "          [0.3841],\n",
       "          [0.4583],\n",
       "          [0.2671],\n",
       "          [0.6128],\n",
       "          [0.4786],\n",
       "          [0.6102],\n",
       "          [0.6577],\n",
       "          [0.4353],\n",
       "          [0.4571],\n",
       "          [0.3366]], grad_fn=<SigmoidBackward0>),\n",
       "  tensor([[0.4460],\n",
       "          [0.6588],\n",
       "          [0.4393],\n",
       "          [0.6087],\n",
       "          [0.6458],\n",
       "          [0.4906],\n",
       "          [0.6662],\n",
       "          [0.5403],\n",
       "          [0.5312],\n",
       "          [0.4761],\n",
       "          [0.5849],\n",
       "          [0.5742],\n",
       "          [0.6435],\n",
       "          [0.6641],\n",
       "          [0.6647],\n",
       "          [0.4585],\n",
       "          [0.5417],\n",
       "          [0.5283],\n",
       "          [0.6605],\n",
       "          [0.4294],\n",
       "          [0.4900],\n",
       "          [0.6886],\n",
       "          [0.7165],\n",
       "          [0.6609],\n",
       "          [0.4934],\n",
       "          [0.7271],\n",
       "          [0.4234],\n",
       "          [0.6486],\n",
       "          [0.4096],\n",
       "          [0.4861],\n",
       "          [0.6816],\n",
       "          [0.5638],\n",
       "          [0.6127],\n",
       "          [0.4007],\n",
       "          [0.5800],\n",
       "          [0.6046],\n",
       "          [0.3825],\n",
       "          [0.5914],\n",
       "          [0.5249],\n",
       "          [0.4637],\n",
       "          [0.6184],\n",
       "          [0.5453],\n",
       "          [0.4658],\n",
       "          [0.4733],\n",
       "          [0.4857],\n",
       "          [0.5889],\n",
       "          [0.4864],\n",
       "          [0.6486],\n",
       "          [0.6052],\n",
       "          [0.5207]], grad_fn=<SigmoidBackward0>),\n",
       "  tensor([[0.5050],\n",
       "          [0.5009],\n",
       "          [0.6103],\n",
       "          [0.6707],\n",
       "          [0.5870],\n",
       "          [0.6340],\n",
       "          [0.5019],\n",
       "          [0.6034],\n",
       "          [0.3063],\n",
       "          [0.5752],\n",
       "          [0.5361],\n",
       "          [0.4430],\n",
       "          [0.4081],\n",
       "          [0.4042],\n",
       "          [0.5211],\n",
       "          [0.5625],\n",
       "          [0.4112],\n",
       "          [0.3749],\n",
       "          [0.4609],\n",
       "          [0.4330],\n",
       "          [0.6415],\n",
       "          [0.5229],\n",
       "          [0.5643],\n",
       "          [0.4434],\n",
       "          [0.5848],\n",
       "          [0.5218],\n",
       "          [0.5058],\n",
       "          [0.5605],\n",
       "          [0.5861],\n",
       "          [0.5307],\n",
       "          [0.5698],\n",
       "          [0.4603],\n",
       "          [0.6568],\n",
       "          [0.5211],\n",
       "          [0.4777],\n",
       "          [0.6919],\n",
       "          [0.4847],\n",
       "          [0.5981],\n",
       "          [0.6647],\n",
       "          [0.4543],\n",
       "          [0.4405],\n",
       "          [0.4667],\n",
       "          [0.5407],\n",
       "          [0.4605],\n",
       "          [0.4388],\n",
       "          [0.3555],\n",
       "          [0.5299],\n",
       "          [0.5340],\n",
       "          [0.4613],\n",
       "          [0.5632]], grad_fn=<SigmoidBackward0>),\n",
       "  tensor([[0.5538],\n",
       "          [0.3049],\n",
       "          [0.4049],\n",
       "          [0.4058],\n",
       "          [0.2526],\n",
       "          [0.4906],\n",
       "          [0.4019],\n",
       "          [0.4169],\n",
       "          [0.3502],\n",
       "          [0.4746],\n",
       "          [0.5110],\n",
       "          [0.3587],\n",
       "          [0.4831],\n",
       "          [0.4644],\n",
       "          [0.5718],\n",
       "          [0.4495],\n",
       "          [0.5867],\n",
       "          [0.4463],\n",
       "          [0.4000],\n",
       "          [0.5336],\n",
       "          [0.5037],\n",
       "          [0.3760],\n",
       "          [0.4330],\n",
       "          [0.4684],\n",
       "          [0.4350],\n",
       "          [0.3580],\n",
       "          [0.4308],\n",
       "          [0.4418],\n",
       "          [0.4140],\n",
       "          [0.4800],\n",
       "          [0.3868],\n",
       "          [0.5566],\n",
       "          [0.5478],\n",
       "          [0.5501],\n",
       "          [0.4517],\n",
       "          [0.5007],\n",
       "          [0.5056],\n",
       "          [0.3112],\n",
       "          [0.3303],\n",
       "          [0.5946],\n",
       "          [0.4102],\n",
       "          [0.6776],\n",
       "          [0.4990],\n",
       "          [0.4830],\n",
       "          [0.6159],\n",
       "          [0.6093],\n",
       "          [0.3436],\n",
       "          [0.6166],\n",
       "          [0.4197],\n",
       "          [0.5501]], grad_fn=<SigmoidBackward0>)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary outside the class\n",
    "MODEL_DICT = {\n",
    "    'densetFPN_121': densetFPN_121,\n",
    "    'densetFPN_201': densetFPN_201,\n",
    "    'efficientFPN_v2_s': efficientFPN_v2_s,\n",
    "    'efficientDecoder_v2_s': efficientDecoder_v2_s\n",
    "}\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, backbone, num_tasks=5, feature_dim=(256, 25, 25)):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.backbone = backbone()  # Instantiate the backbone passed as a class\n",
    "        \n",
    "        feature_channels, feature_width, feature_height = feature_dim\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((feature_width, feature_height))\n",
    "        \n",
    "        self.task_specific_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(feature_channels * feature_width * feature_height, 1024),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        self.task_specific_classifier = nn.ModuleList([\n",
    "            nn.Linear(1024, 1) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(1024 * num_tasks, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        intermediate_outputs = [layer(x) for layer in self.task_specific_layers]\n",
    "        concatenated_outputs = torch.cat(intermediate_outputs, dim=1)\n",
    "        \n",
    "        task_outputs = [torch.sigmoid(classifier(io)) for io, classifier in zip(intermediate_outputs, self.task_specific_classifier)]\n",
    "        \n",
    "        final_output = torch.sigmoid(self.final_classifier(concatenated_outputs))\n",
    "        \n",
    "        return final_output, task_outputs\n",
    "\n",
    "def construct_baselineModel(model_name, num_tasks=5, feature_dim=(256, 25, 25)):\n",
    "    if model_name not in MODEL_DICT:\n",
    "        raise ValueError(f\"Unsupported model name {model_name}\")\n",
    "    backbone = MODEL_DICT[model_name]\n",
    "    return BaselineModel(backbone, num_tasks, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prototype_shape[0]: 2000\n",
      "prototype_shape[1]: 512\n",
      "prototype_shape[2]: 1\n",
      "prototype_shape[3]: 1\n"
     ]
    }
   ],
   "source": [
    "prototype_shape=(2000, 512, 1, 1)\n",
    "for i in range(0, len(prototype_shape)):\n",
    "    print(f\"prototype_shape[{i}]: {prototype_shape[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  90456198\n",
      "Backbone output shape: torch.Size([50, 512, 1, 1])\n",
      "Prototype 0 shape: torch.Size([2000, 512, 1, 1])\n",
      "Unsqueeze prototype shape: torch.Size([1, 2000, 512, 1, 1]), x shape: torch.Size([50, 512, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (50) must match the size of tensor b (2000) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Forward pass through the model with dummy input\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m final_logits, task_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Print output shapes to verify\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal logits shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_logits\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32md:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 62\u001b[0m, in \u001b[0;36mHybridProtoPNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# print(f\"Task index: {task_index}\")\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsqueeze prototype shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprototype\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, x shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistances shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistances\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_specific_layers[task_index](distances)\n",
      "File \u001b[1;32md:\\Program_Files\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\functional.py:1336\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1334\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode)\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (50) must match the size of tensor b (2000) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.models.backbone_models import denseFPN_121, denseFPN_201, efficientFPN_v2_s, efficientDecoder_v2_s\n",
    "\n",
    "# Dictionary of supported backbone models\n",
    "BACKBONE_DICT = {\n",
    "    'denseFPN_121': denseFPN_121,\n",
    "    'denseFPN_201': denseFPN_201,\n",
    "    'efficientFPN_v2_s': efficientFPN_v2_s,\n",
    "    'efficientDecoder_v2_s': efficientDecoder_v2_s\n",
    "}\n",
    "\n",
    "class HybridProtoPNet(nn.Module):\n",
    "    def __init__(self, backbone, hidden_layers, num_tasks, num_prototypes_per_task, prototype_shape):\n",
    "        super(HybridProtoPNet, self).__init__()\n",
    "        # Initialize the backbone with specified configurations\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        # Prototype layers for each task\n",
    "        self.prototype_vectors = nn.ParameterList([\n",
    "            nn.Parameter(torch.rand(prototype_shape), requires_grad=True)\n",
    "            for _ in range(num_tasks * num_prototypes_per_task)\n",
    "        ])\n",
    "        \n",
    "        # Task-specific layers to process prototype information\n",
    "        self.task_specific_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(num_prototypes_per_task * prototype_shape[1], hidden_layers),\n",
    "                nn.BatchNorm1d(hidden_layers),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # Classifiers for each task based on processed prototype information\n",
    "        self.task_specific_classifier = nn.ModuleList([\n",
    "            nn.Linear(hidden_layers, 1) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # Final classifier that takes concatenated task outputs to make a final prediction\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_layers * num_tasks, hidden_layers),\n",
    "            nn.BatchNorm1d(hidden_layers),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_layers, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)  # Feature extraction using the backbone\n",
    "        print(f\"Backbone output shape: {x.shape}\")\n",
    "        \n",
    "        # Computing prototype distances and processing them per task\n",
    "        intermediate_outputs = []\n",
    "        for i, prototype in enumerate(self.prototype_vectors):\n",
    "            print(f\"Prototype {i} shape: {prototype.shape}\")\n",
    "            task_index = i // len(self.prototype_vectors) * len(self.task_specific_layers)\n",
    "            # print(f\"Task index: {task_index}\")\n",
    "            print(f\"Unsqueeze prototype shape: {prototype.unsqueeze(0).shape}, x shape: {x.shape}\")\n",
    "            distances = torch.cdist(x, prototype.unsqueeze(0), p=2)\n",
    "            print(f\"Distances shape: {distances.shape}\")\n",
    "            processed = self.task_specific_layers[task_index](distances)\n",
    "            intermediate_outputs.append(processed)\n",
    "        \n",
    "        concatenated_outputs = torch.cat(intermediate_outputs, dim=1)  # Concatenating outputs for final prediction\n",
    "        \n",
    "        # Predicting binary outputs for each task\n",
    "        task_outputs = [torch.sigmoid(self.task_specific_classifier[i](intermediate_outputs[i])) for i in range(len(self.task_specific_layers))]\n",
    "        \n",
    "        # Final malignancy or comprehensive prediction\n",
    "        final_output = torch.sigmoid(self.final_classifier(concatenated_outputs))\n",
    "        \n",
    "        return final_output, task_outputs\n",
    "    \n",
    "def construct_ppnetModel(backbone_name='denseNet121', \n",
    "                            weights='DEFAULT', \n",
    "                            common_channel_size=256, \n",
    "                            output_channel_size=256, \n",
    "                            output_feature_size=25, \n",
    "                            hidden_layers=256, \n",
    "                            num_tasks=5,\n",
    "                            num_prototypes_per_task=10,\n",
    "                            prototype_shape=(2000, 512, 1, 1)):\n",
    "    \n",
    "    if backbone_name not in BACKBONE_DICT:\n",
    "        raise ValueError(f\"Unsupported model name {backbone_name}\")\n",
    "    backbone = BACKBONE_DICT[backbone_name](weights=weights, common_channel_size=common_channel_size, output_channel_size=output_channel_size, output_feature_size=output_feature_size)\n",
    "    \n",
    "    return HybridProtoPNet(backbone=backbone, \n",
    "                           weights=weights, \n",
    "                           common_channel_size=common_channel_size, \n",
    "                           output_channel_size=output_channel_size, \n",
    "                           output_feature_size=output_feature_size,\n",
    "                           hidden_layers=hidden_layers, \n",
    "                           num_tasks=num_tasks,\n",
    "                           num_prototypes_per_task=num_prototypes_per_task,\n",
    "                           prototype_shape=prototype_shape)\n",
    "\n",
    "model = construct_ppnetModel(backbone_name='denseFPN_121', weights='DEFAULT', common_channel_size=256, output_channel_size=512, output_feature_size=1, hidden_layers=1024, num_tasks=5, num_prototypes_per_task=10, prototype_shape=(2000, 512, 1, 1))\n",
    "\n",
    "\n",
    "# Print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "# Create a dummy input tensor of size [50, 3, 100, 100]\n",
    "dummy_input = torch.randn(50, 3, 100, 100)\n",
    "\n",
    "# Forward pass through the model with dummy input\n",
    "final_logits, task_logits = model(dummy_input)\n",
    "\n",
    "# Print output shapes to verify\n",
    "print(\"Final logits shape:\", final_logits.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
